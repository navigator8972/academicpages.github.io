@string{icra = {Proceedings of the IEEE International Conference on Robotics and
    Automation (ICRA)}}
@string{icra_abbrev = {Proc.\ IEEE Int'l Conf.\ on Robotics and
    Automation (ICRA)}}
@string{wafr = {Proceedings of the International Workshop on Algorithmic Foundations
    of Robotics (WAFR)}}
@string{wafr_abbrev = {Proc.\ Int'l Workshop on Algorithmic Foundations
    of Robotics (WAFR)}}
@string{iros = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}
@string{iros_abbrev = {Proc. IEEE/RSJ Int'l Conf.\ on Intelligent Robots and Systems (IROS)}}
@string{iser = {Proceedings of the International Symposium on Experimental Robotics (ISER)}}
@string{iser_abbrev = {Proc.\ Int'l. Symp.\ on Experimental Robotics (ISER)}}
@string{isrr = {Proceedings of the International Symposium of Robotics Research (ISRR)}}
@string{isrr_abbrev = {Proc.\ Int'l.\ Symp.\ of Robotics Research (ISRR)}}
@string{cvprcs="Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{cvprcs_abbrev="Proc.\ IEEE Comp.\ Soc.\ Conf.\ on Computer Vision and Pattern Recognition (CVPR)"}
@string{cvpr="Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{cvpr_abbrev="Proc.\ IEEE Conf.\ on Computer Vision and Pattern Recognition (CVPR)"}
@string{iccv="Proceedings of the International Conference on Computer Vision (ICCV)"}
@string{iccv_abbrev="Proc.\ Int'l.\ Conf.\ on Computer Vision (ICCV)"}
@string{ijcv="International Journal on Computer Vision"}
@string{ijcv_abbrev="Int'l J.\ on Computer Vision"}
@string{ijrr="International Journal of Robotics Research"}
@string{ijrr_abbrev="Int'l J.\ of Robotics Research"}
@string{hri="Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI)"}
@string{hri_abbrev="Proc.\ Int'l. Conf.\ on Human-Robot Interaction"}
@string{rss="Proceedings of Robotics: Science and Systems (RSS)"}
@string{rss_abbrev="Proc.\ Robotics: Science and Systems (RSS)"}
@string{aaai="Proceedings of the National Conference on Artificial Intelligence (AAAI)"}
@string{aaai_abbrev="Proc.\ Nat'l Conf.\ on Artificial Intelligence (AAAI)"}
@string{tro="Transactions on Robotics"}
@string{tro_abbrev="Trans.\ on Robotics"}
@string{auro="Autonomous Robots"}
@string{ras="Robotics and Autonomous Systems"}
@string{ijcai="Proceedings of International Joint Conference on Artificial Intelligence"}
@string{uai="Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)"}
@string{uai_abbrev="Proc.\ Conf.\ on Uncertainty in Artificial Intelligence (UAI)"}
@string{icml="Proceedings of the International Conference on Machine Learning (ICML)"}
@string{icml_abbrev="Proc.\ Int'l Conf.\ on Machine Learning (ICML)"}
@string{nips="Proceedings of Advances in Neural Information Processing Systems (NIPS)"}
@string{humd="Proceedings of {IEEE} International Conference on Humanoid Robots ({H}umanoids)"}
@string{humd_abbrev="Proc. {IEEE} Intl Conf. on Humanoid Robots ({H}umanoids)"}
@string{icdl="Proceedings of the International Conference on Development and Learning ({ICDL-EpiRob})"}
@string{icdl_abbrev="Proc. Intl Conf. on Development and Learning ({ICDL-EpiRob})"}
@string{jmlr="Journal of Machine Learning Research"}
@string{aamas="Proceedings of the International Conference on Autonomous Agents and MultiAgent Systems ({AAMAS})"}
@string{aamas_abbrev="Proc. Intl Conf. on Autonomous Agents and MultiAgent Systems ({AAMAS})"}
@string{nips="Proceedings of Neural Information Processing Systems ({NIPS})"}
@string{iclr="Proceedings of the International Conference on Learning Representations ({ICLR})"}
@string{corl="Proceedings of Machine Learning Research: Conference on Robot Learning ({CoRL})"}

@INPROCEEDINGS{Miao2014, 
  
   author={Li, M. and Yin, H. and Tahara, K. and Billard, A.}, 
  
   booktitle=icra, 
   year=2014,
   title={Learning Object-level Impedance Control for Robust Grasping and
  
   Dexterous Manipulation},
   address = {Hong Kong, China},
   url = {http://lasa.epfl.ch/publications/uploadedFiles/ICRA2014hand.pdf},
}

@inproceedings{hyin2014Hum,

author="Yin, H. and Paiva, A. and Billard, A",

title="Learning Cost Function and Trajectory for Robotic Writing Motion",

booktitle=humd,

year="2014",

address="Madrid, Spain",

url = {http://lasa.epfl.ch/publications/uploadedFiles/humanoids14_hyin_final.pdf},
}

@INPROCEEDINGS{Yin2016-ID961,

  author       = {Yin, H. and Alves-Olivera, P. and Melo, F. S. and Billard, A. and Paiva, A.},

  title        = {Synthesizing Robotic Handwriting Motion by Learning from Human Demonstrations},

  booktitle = {Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)},

  year         = {2016},
	address	   ="New York, USA",
  abstract     = {This paper contributes a novel framework that enables a robotic agent to efficiently learn and synthesize believable handwriting motion. We situate the framework as a foundation with the goal of allowing children to observe, correct and engage with the robot to learn themselves the handwriting skill. The framework adapts the principle behind ensemble methods - where improved performance is obtained by combining the output of multiple simple algorithms - in an inverse optimal control problem. This integration addresses the challenges of rapid extraction and representation of multiple-mode motion trajectories, with the cost forms which are transferable and interpretable in the development of the robot compliance control. It also introduces the incorporation of a human movement inspired feature, which provides intuitive motion modulation to generalize the synthesis with poor robotic written samples for children to identify and correct. We present the results on the success of synthesizing a variety

of natural-looking motion samples based upon the learned cost functions. The framework is validated by a user study, where the synthesized dynamical motion is shown to be hard to distinguish from the real human handwriting.},

 url ={http://lasa.epfl.ch/publications/uploadedFiles/hyin_ijcai16_2277.pdf},
}

@inproceedings{Yin2015-ID962,

  author       = {Yin, H. and Billard, A. and Paiva, A.},

  title        = {Bidirectional Learning of Handwriting Skill in Human-Robot Interaction},

  booktitle = {Proceedings of ACM/IEEE International Conference on Human-Robot Interaction (HRI): HRI Pioneer Workshop},

  year         = {2015},

  abstract     = {This paper describes the design of a robot agent and associated learning algorithms to help children in handwriting acquisition. The main issue lies in how to program a robot to

obtain human-like handwriting and then exploit it to teach children. We propose to address this by integrating learning from demonstrations paradigm, which allows the robot

to extract a task index from intuitive expert (e.g., adults) demonstrations. We present our work on the development of an algorithm, as well as its validation by learning compliant robotic writing motion from the extracted index. Also discussed is the synthesis of the learned task in the prospective work of transferring the task skill to users, especially in

terms of learning by teaching. The undergoing work about the design of a sensor-embedded pen is introduced. This will be used as an intuitive interface for recording various handwriting related information in the interaction.},

 url ={http://lasa.epfl.ch/publications/uploadedFiles/hyin_hripa2015_180.pdf},
}

@inproceedings{Yin2017-ID976,

  author       = {Yin, H. and Melo, F. S. and Billard, A. and Paiva, A.},

  title        = {Associate Latent Encodings in Learning from Demonstrations},

  booktitle = aaai,

  year         = {2017},
  address	   ="San Francisco, USA",

 url ={http://lasa.epfl.ch/publications/uploadedFiles/hyin_aaai17_641_compressed.pdf},
}

@inproceedings { shruti_aamas17,
	abstract = {The focus of the research described in this paper is to explore children's perception of a social robot's learning abilities and behaviour in an educational context. With this purpose, we conducted a long-term study with children in a school by adopting the learning-by-teaching learning method. The scenario involves a "learner-agent" (a robot) which seeks help from a child (a teacher) in correcting the shapes of a few letters it writes. Two versions of the robot were built: one where it learns and another where it does not improve over time. The results of the study suggest that children's social relationship with the robot was not affected by the learning abilities of the agent.  },
	keywords = {Social Robotic Companions;},
    booktitle = aamas,
	month = {May},
	pages = {1490-1492},
	title = {Affect of Robot's Competencies on Children's Perception.},
	year = {2017},
	author = {Chandra, Shruti and Paradeda, Raul and Yin, Hang and Dillenbourg, Pierre  and Prada, Rui  and Paiva, Ana},

    url = {{https://github.com/navigator8972/navigator8972.github.io/raw/master/files/AffectOfRobotCompetency.pdf}},
}

@article{Yin2018-ID1003,

  author       = {Yin, H. and Melo, F. S. and Paiva, A. and Billard, A.},

  title        = {An Ensemble Inverse Optimal Control Approach for Robotic Task Learning and Adaptation},

  journal = auro,

  year         = {2019},

  abstract     = {This paper contributes a novel framework to efficiently learn cost-to-go function representations for robotic tasks with latent modes. The proposed approach relies on the principle behind ensemble methods, where improved performance is obtained by aggregating a group of simple models, each of which can be efficiently learned. The maximum-entropy approximation is adopted as an effective initialization and the quality of this surrogate is guaranteed by a theoretical bound. Our approach also provides an alternative perspective to view the popular mixture of Gaussians under the framework of inverse optimal control. We further propose to enforce a dynamics on the model ensemble, using Kalman estimation to infer and modulate model modes. This allows robots to exploit the demonstration redundancy and to adapt to human interventions, especially in tasks where sensory observations are non-Markovian. The framework is demonstrated with a synthetic inverted pendulum example and online adaptation tasks, which include robotic handwriting and mail delivery. },

    url = {https://infoscience.epfl.ch/record/255160/files/paper.pdf},
}

@INPROCEEDINGS{Chandra2018-ID1005,

  author       = {Chandra, S. and Paradeda, R. and Yin, H. and Dillenbourg, P. and Prada, R. and Paiva, A.},

  title        = {Do Children Perceive Whether a Robotic Peer is Learning or Not},

  booktitle = hri,

  year         = {2018},

  abstract     = {},

  url = {{https://github.com/navigator8972/navigator8972.github.io/raw/master/files/LearnOrNotHRI18.pdf}},
}

@INPROCEEDINGS{Yin2018-ID1006,

  author       = {Yin, H. and Melo, F. S. and Billard, A. and Paiva, A.},

  title        = {Boosting Robot Learning and Control with Domain Constraints},

  booktitle = {Proceedings of Robotics: Science and Systems (RSS), Pioneer Workshop},

  year         = {2018},

  abstract     = {},

    url = {http://lasa.epfl.ch/publications/uploadedFiles/hyin_rsspioneer.pdf},
}


@phdthesis{Yin2018-ID1017,

  author       = {Yin, H.},

  title        = {Incorporating Human Expertise in Robot Motion Learning and Synthesis},

  school = {EPFL and IST, University of Lisbon},

  year         = {2018},

  url = {{https://github.com/navigator8972/navigator8972.github.io/raw/master/files/thesis_ist_final.pdf}},
  abstract     = {With the exponential growth of robotics and the fast development of their advanced cognitive and motor capabilities, one can start to envision humans and robots jointly working together in unstructured environments. Yet, for that to be possible, robots need to be programmed for such types of complex scenarios, which demands significant domain knowledge in robotics and control. One viable approach to enable robots to acquire skills in a more flexible and efficient way is by giving them the capabilities of autonomously learn from human demonstrations and expertise through interaction. Such framework helps to make the creation of skills in robots more social and less demanding on programing and robotics expertise. Yet, current imitation learning approaches suffer from significant limitations, mainly about the flexibility and efficiency for representing, learning and reasoning about motor tasks. This thesis addresses this problem by exploring cost-function-based approaches to learning robot motion control, perception and the interplay between them. To begin with, the thesis proposes an efficient probabilistic algorithm to learn an impedance controller to accommodate motion contacts. The learning algorithm is able to incorporate important domain constraints, e.g., about force representation and decomposition, which are nontrivial to handle by standard techniques. Compliant handwriting motions are developed on an articulated robot arm and a multi-fingered hand. This work provides a flexible approach to learn robot motion conforming to both task and domain constraints. Furthermore, the thesis also contributes with techniques to learn from and reason about demonstrations with partial observability. The proposed approach combines inverse optimal control and ensemble methods, yielding a tractable learning of cost functions with latent variables. Two task priors are further incorporated. The first human kinematics prior results in a model which synthesizes rich and believable dynamical handwriting. The latter prior enforces dynamics on the latent variable and facilitates a real-time human intention cognition and an on-line motion adaptation in collaborative robot tasks. Finally, the thesis establishes a link between control and perception modalities. This work offers an analysis that bridges inverse optimal control and deep generative model, as well as a novel algorithm that learns cost features and embeds the modal coupling prior. This work contributes an end-to-end system for synthesizing arm joint motion from letter image pixels. The results highlight its robustness against noisy and out-of-sample sensory inputs. Overall, the proposed approach endows robots the potential to reason about diverse unstructured data, which is nowadays pervasive but hard to process for current imitation learning.},

}

@techreport {Yin2018-ID1018,
	abstract = {In this paper, we learn dynamics from highdimensional
demonstrations to facilitate model-based prediction
and robot control. The proposed approach leverages the
progress in variational-bayes and sequence modeling, extracting
a low-dimensional latent space so the dynamical relations of
interest can be compactly represented and learned. Different
from existing works, our model captures latent dynamics
in a more general form and features efficient inference for
pattern filtering, prediction and synthesis. The extracted feature
mapping and latent dynamics can be naturally integrated in
robot learning, yielding task imitation from raw data and
prediction-based reproduction. The performance of latent dynamics
learning and model-based imitation is shown in three
tasks: 1) reconstructing and predicting images of bouncing balls
movement with an accuracy competitive to the state-of-the art;
2) synthesizing diverse handwriting image sequences; 3)
learning to strike a ball under partial visual input, with results
significantly outperforming baselines.},
	institution = {Intelligent Agents and Synthetic Characters Group},
	month = {May},
	number = {GAIPS-TR-001-17},
	title = {Learning Variational Latent Dynamics: Towards Model-based Imitation and Control},
	year = {2018},
	author = {Yin, H. and Melo, F. S. and Billard, A. and Paiva, A.},

    url = {{https://github.com/navigator8972/navigator8972.github.io/raw/master/files/learnvaedyn2018.pdf}},
}

@ARTICLE{benchmark2020ral,
  author={I. {Garcia-Camacho} and M. {Lippi} and M. C. {Welle} and H. {Yin} and R. {Antonova} and A. {Varava} and J. {Borras} and C. {Torras} and A. {Marino} and G. {Alenyà} and D. {Kragic}},
  journal={IEEE Robotics and Automation Letters}, 
  title={Benchmarking Bimanual Cloth Manipulation}, 
  year={2020},
  volume={5},
  number={2},
  pages={1111-1118},}
 
 
@ARTICLE{khader2019dataefficient,
    title={Data-efficient Model Learning and Prediction for Contact-rich Manipulation Tasks},
    author={Shahbaz Abdul Khader and Hang Yin and Pietro Falco and Danica Kragic},
    journal={IEEE Robotics and Automation Letters}, 
    year={2020},
}

@INPROCEEDINGS{onlineDMG2020icra,
   author = {Cruciani, Silvia and Yin, Hang and Kragic, Danica},

   title = {In-Hand Manipulation of Objects with Unknown Shapes},
   abstract = {This work addresses the problem of changing grasp configurations on objects with an unknown shape through in-hand manipulation. Our approach leverages shape priors,learned as deep generative models, to infer novel object shapesfrom partial visual sensing. The Dexterous Manipulation Graph method is extended to build upon incremental data and account for estimation uncertainty in searching a sequence of manipulation actions. We show that our approach successfully solves in-hand manipulation tasks with unknown objects, and demonstrate the validity of these solutions with robot experiments. },
   booktitle = icra,
   year = {2020}
}

﻿@INPROCEEDINGS{lsr2020iros,
   author = {Martina Lippi and Petra Poklukar and Michael C Welle and Anastasiia Varava and Hang Yin and Alessandro Marino and Danica Kragic},

   title = {Latent Space Roadmap for Visual Action Planning of Deformable and Rigid Object Manipulation},
   booktitle = iros,
   year = {2020}
}

@ARTICLE{khader2019stablecontact,
    title={Stability-Guaranteed Reinforcement Learning for Contact-rich Manipulation},
    author={Shahbaz Abdul Khader and Hang Yin and Pietro Falco and Danica Kragic},
    journal={IEEE Robotics and Automation Letters}, 
    year={2020},
}
